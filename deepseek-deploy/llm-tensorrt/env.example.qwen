# TensorRT-LLM集群配置环境变量示例
# 复制此文件到.env并根据需要修改

# Docker配置
DOCKER_IMAGE="baseten/tensorrt_llm-release:0.19.0rc0"

# SSH配置
SSH_PORT=2233

# 目录配置
# 免密登录秘钥路径
SSHKEY_DIR="./sshkey"

# 共享存储中模型文件的根路径
MODEL_REPO_DIR="/modelshare_readonly/qwen"

# 节点配置
# 设置为true表示当前节点是主节点，将安装mpich
# 设置为false表示当前节点是从节点，不安装mpich
IS_MASTER=true
MASTER_ADDR=10.83.0.101
MASTER_PORT=29500

# IB网络
GLOO_SOCKET_IFNAME=bond0
NCCL_SOCKET_IFNAME=bond0
NCCL_IB_HCA=mlx5_0,mlx5_1,mlx5_4,mlx5_5

# TensorRT-LLM服务相关配置
# 推理引擎后端 pytorch | tensorrt
TRT_BACKEND=pytorch
# 模型路径
MODEL_PATH="/workspace/Qwen2.5-72B-Instruct/"
# 启动进程数量
NUM_PROCESSES=8
# Tensor并行度
TP_SIZE=8
PP_SIZE=1
# Expert并行度
EP_SIZE=1
# 最大批处理大小
MAX_BATCH_SIZE=5
# 最大Token数
MAX_NUM_TOKENS=2048
# KV缓存占GPU内存比例
KV_CACHE_FRACTION=0.7
# 额外配置文件名
ENABLE_EXTRA_CONFIG=false
EXTRA_CONFIG="extra-llm-api-config.yml"

# 服务端口
SERVER_PORT=8888
# 是否后台启动,后台启动后ctrl+c容器不会退出
RUN_IN_BACKGROUND=false
# 运行的日志路径
LOG_FILE=/var/log/trt-llm/server.log